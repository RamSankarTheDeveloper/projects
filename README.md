

|**Type**|**Project**|**Dataset**|**Algorithms with score**|**Libraries**|**methodology**|**What the model/program does**|
| :- | :- | :- | :- | :- | :- | :- |
|<p>NLP with DL </p><p></p><p></p><p>` `(current project)</p>|[Question generator based on Bloom’s taxonomy](https://github.com/RamSankarTheDeveloper/projects/blob/main/resume word count analysis.ipynb#https://github.com/RamSankarTheDeveloper/projects/blob/main/resume word count analysis.ipynb)(ongoing)|<p>NLP, dataset created by self,</p><p>eli5 dataset</p>|Purely NLP as of yet, is adding deep learning and pretrained language models.|spacy, nltk, neuralcoref, pandas, NumPy, keybert, parrot, regex as of yet|<p>- Program to preprocess entered text</p><p>- Program to determine entities using neural coref, NER, repeated POS patterns etc.</p><p>- Program to blank out extracted entities to generate fill-in-the-blank questions (level 1 question on Bloom’s taxonomy)</p><p>- Program to preprocess sentences that have keywords and generate questions associating the entities and keyword (level 3 questions)</p><p>- Program to train pre trained language models to generate further complex questions</p>|<p>- Takes in a textbook and generates questions of 5 difficulty/complexity levels in Bloom’s taxonomy</p><p>- Currently (NLP only version) the model can generate level 1 questions and partially level 3 questions.</p>|
|DL|[Dog-cat classification](https://github.com/RamSankarTheDeveloper/projects/blob/main/5.deep learning/CNN/CNN_practical.ipynb)|stock images|<p>CNN</p><p>(WA=0.79)</p>|sklearn, tqdm, random, cv2, os, tensorflow, keras, matplotlib, seaborn|<p>- Reads and preprocess image folders into train-test variables</p><p>- Creates a CNN black-box model, trained, plotted loss and accuracy graphs</p>|- Given the image of a cat or a dog, the model predicts whether if it is a cat or dog |
|DL|[Churn prediction](https://github.com/RamSankarTheDeveloper/projects/blob/main/5.deep learning/ANN/2.ANN tutorial churn prediction/ANN_churn_prediction.ipynb)|Churn\_modelling dataset from kaggle|<p>ANN</p><p>(84.25)</p>|tensorflow, keras, sklearn, numpy, pandas, matplotlib|<p>- Dataset analysis and preprocessing</p><p>- Created an ANN black-box model and trained</p><p>- Analysed the accuracy of  the model</p>|- Given numerical data-point about a customer, the model predicts the probability of the customer terminating his/her account from the bank|
|DL|[Handwritten digit recognition](https://github.com/RamSankarTheDeveloper/projects/tree/main/5.deep learning/ANN/3.handwritten digits)|Mnist dataset from keras|<p>ANN</p><p>(65.68)</p>|keras, numpy, matplotlib|<p>- Created a black box model, trained using dataset, error happened due to incompatible shapes</p><p>- Reshaped using one hot encoding</p><p>- Trained successfully</p><p>- Evaluated the model</p>|- Given pixel values in 28\*28 resolution (B&W) of an image of handwritten number(single digit), the model will predict the number|
|DL|[Diabetics prediction](https://github.com/RamSankarTheDeveloper/projects/tree/main/5.deep learning/ANN/1.diabetis)|Diabetics dataset from Kaggle(USA, before 1990)|<p>ANN</p><p>(74.61)</p>|keras, pandas, numpy|<p>- Created an ANN black-box model and trained it</p><p>- Analysed the accuracy of  the model</p>|- Given the numerical data, determines the probability that the patient has diabetics and not|
|NLP|[Hate speech detection](https://github.com/RamSankarTheDeveloper/projects/tree/main/4.Natural Language processing/twitter hate speech)|Twitter data|Multinomial naive bayes(95.25), Random forest classifier(f1=98.33) , XGBoost classifier(96.38),SVM(98.53), Logistic regression(97.41)|sklearn and other data analysis libraries|<p>- Did analysis and preprocessing</p><p>- Applied Regex to clean data</p><p>- Applied oversampling (random)</p><p>- Applied Tfidftransformer  to xtrain after applying Count-vectorizer</p><p>- Did multiple algorithms and analysed each model’s F1 score</p><p>- Analysed ROC curve, Precision-recall curve for Random forest and XGBoost</p>|- Given comment/sentence, the model will predict if it is hate speech or not (1 if hate speech, 0 if not)|
|NLP|[Sentiment analysis using text classification](https://github.com/RamSankarTheDeveloper/projects/tree/main/4.Natural Language processing/sentiment analysis project)|Sentiment analysis of movie reviews dataset from Kaggle|<p>Multinomial naive bayes</p><p>(58.65)</p>|sklearn, pandas, matplotlib|<p>- Dataset analysis</p><p>- Applied Tfidfvectorizer</p><p>- Fitted and tested the model</p>|- Given a review that was preprocessed with Tfidfvectorizer, the model will predict: 0 if negative, 1 if somewhat negative, 2 if neutral, 3 if somewhat positive, 4 if positive|
|NLP|[Email spam classification](https://github.com/RamSankarTheDeveloper/projects/tree/main/2.ML/Supervised learning/others)|Smsspamcollection dataset|<p>Multinomial naive bayes</p><p>(WA of F1=0.98)</p>|sklearn, joblib, pandas, imblearn|<p>- Preprocessed dataset with Encoding, Countvectorizer, Random sampling</p><p>- Fitted the model</p><p>- Analysed the model with Classification matrix</p><p>- Saved the model in Joblib format</p>|- Preprocess a sample into countvectorizer feed to model and the model will predict if it is a spam comment or not|
|Computervision|[Face detector & recognizer](https://github.com/RamSankarTheDeveloper/projects/tree/main/3.computer vision/face_recognition)|Self generated data |HAAR cascade pretrained model, LBPH face recognizer|cv2, os, csv, numpy, PIL, pandas|<p>- Created folders ‘id’ and ‘name’</p><p>- Took face samples directly from video feed detected by HAAR cascade detector and stored as file</p><p>- Created a subdirectory for trainer data, cropped out the faces using haar, trained using LBPH Recognizer</p><p>- Saved the trained model(yml file) into subdirectory,’trainer’</p><p>- Tested the model with opening Camera,  preprocessed the feed, and analysed using c-Confidence of the Recognizer.</p>||
|<p>ML</p><p></p>|[Customer segmentation](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Unsupervised learning/clustering/DBSCAN VISUALIZED detailed.ipynb)|Mall\_customers dataset from Kaggle|K-means clustering, DBSCAN clustering, Agglomerative clustering, Hierarchical clustering|sklearn, pandas, matplotlib, numpy, math|<p>- Feature engineered necessary columns</p><p>- Fitted K-means algorithm 10 times with 10 values</p><p>- Plotted elbow method graph (wcss list vs number of clusters) and chose optimum number of clusters</p><p>- Visualised clusters</p><p>- Used nearest neighbours to determine  optimum parameters with K-distance graph, fitted on DBSCAN  and visualised the clusters</p><p>- Fitted Agglomerative and Hierarchical clustering additionally</p>|- Given income, it will predict the cluster the person is a part of.|
|<p>ML</p><p></p>|[Market basket  analysis of store data](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Unsupervised learning/association/apriori algorithm of market basket analysis.ipynb)|7500 transactions of a week at a French retail store|Apriori|apyori, numpy, pandas, matplotlib|<p>- Data analysis and preprocessing</p><p>- Fitted Apriori model with required parameters</p><p>- Derived relations and created rules</p>|- The model created some rules. You can analyse the relations of items with other items.|
|ML|[Iris multiclass classification](https://github.com/RamSankarTheDeveloper/projects/tree/main/2.ML/Supervised learning/classification/multiclass classification)|Iris dataset(the characteristics of each iris species flowers)|Decision trees(0.93), KNN(0.93), Random forest(0.93), Gradient boosting regressor(0.93)|matplotlib, seaborn, pandas, numpy|<p>- Data analysis and visualisation</p><p>- Trained decision tree model, visualised tree, evaluated, hyper-parameter tuned with K fold and GridSearchCV</p><p>- Trained KNN, Random forest, Gradient boosting regressor models</p><p>- Hyper-parameter tuned the above models</p><p>- Evaluated the above models with multi-label confusion matrix and classification report.</p>|- Given independent variables, models will predict the class the sample datapoint belongs in.|
|ML|[Breast cancer prediction](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Supervised learning/classification/binary classification/rest of binary classifications/rest of binary classifications.ipynb)|Breast cancer dataset|Logistic regression(acc=95.80), SVM(93.7), KNN(95.1), Decision tree(90.9), Naive bayes(93.7)|sklearn, matplotlib, pandas|<p>- Dataset preprocessing</p><p>- Trained multiple models</p><p>- Evaluated all the models and visually compared the classification reports.</p>|- Given independent variables, models will predict the  class the sample data-point belongs in.|
|ML|[Predicting survival Titanic passengers](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Supervised learning/classification/binary classification/logistic regression/logistic regression.ipynb)|Titanic dataset|Logistic regression (79.88)|sklearn, pandas|<p>- Dataset analysis and preprocessing</p><p>- Fitted Logistic regression and analysed the model</p>|- given the independent variables of passengers in Titanic dataset, it will predict whether they survive or not|
|ML|[Predicting car purchase customers](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Supervised learning/classification/binary classification/logistic regression/logistic regression 2.ipynb)|SUV dataset(whether the customer buys SUV or not)|<p>Logistic regression</p><p>(WA= 0.83)</p>|sklearn, pandas, matplotlib, seaborn|<p>- Dataset analysis and preprocessing</p><p>- Logistic model is created and fitted</p><p>- Analysed with Classification matrix</p><p>- Plotted Roc-auc curve</p><p>- Calculated Log loss(0.39)</p>|- Given the independent variables of customers, the model will predict whether they will buy the car or not|
|ML|[Brain weight prediction](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Supervised learning/regression/linear regression.ipynb)|Headbrain.csv|<p>Linear regression</p><p>(R2=0.63)</p>|sklearn, numpy, pandas|<p>- Dataset preprocessing</p><p>- Fitted into Linear regression model</p><p>- Evaluated score</p>|- Given input variables, model will try to predict a value|
|ML|[Polynomial regression from scratch witout  vectorization](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Supervised learning/regression/polynomial self.ipynb)|Headbrain.csv|<p>Polynomial regression without dot product</p><p>(R2=0.58)</p>|sklearn, numpy, pandas, matplotlib|<p>- Converted data into input and output by code</p><p>- Standardized independent values by code</p><p>- Executed program and Regression line visualized with matplotlib</p><p>- Evaluated model with Error-iteration plot</p><p>- Calculated R2 score</p>|- Given input variables, model will try to predict a value|
|Data analysis projects|||||||


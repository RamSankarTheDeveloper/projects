

|**Type**|**Name**|**Dataset** |**Algorithms and score**|**Libraries**|**What I did in the project**|**What the model/program does**|
| :- | :- | :- | :- | :- | :- | :- |
|NLP (current project)|[Question generator based on Bloom’s taxonomy](https://github.com/RamSankarTheDeveloper/projects/blob/main/resume word count analysis.ipynb) [(ongoing)](https://github.com/RamSankarTheDeveloper/Question-generation-algorithm-based-on-blooms-taxonomy/tree/main)|NLP, dataset created by self|Purely NLP as of yet, is adding deep learning and pretrained language models.|spaCy, NLTK, neuralcoref, pandas, NumPy, keybert, parrot, regex as of yet|<p>- Program to preprocess entered text</p><p>- Program to determine entities using neural coref, NER, repeated POS patterns etc</p><p>- Program to blank out extracted entities to generate fill-in-the-blank questions (level 1 question on Bloom’s taxonomy)</p><p>- Program to preprocess sentences that have keywords and generate questions associating the entities and keyword (level 3 questions)</p><p>- Program to train pretrained language models to generate further complex questions</p>|<p>- Takes in a textbook and generates questions that of 5 difficulty/complexity levels according to Bloom’s taxonomy</p><p>- Currently (NLP only version) the model can generate level 1 questions and partially level 3 questions.</p>|
|DL|[Dog-cat classification](https://github.com/RamSankarTheDeveloper/projects/blob/main/5.deep learning/CNN/CNN_practical.ipynb)|stock images|<p>CNN2</p><p>(WA=0.79)</p>|sklearn, tqdm, random, cv2, os, tensorflow, keras, matplotlib, seaborn|<p>reads and preprocess image folders into train and test variables</p><p>create a CNN blackbox model</p><p>callbacks</p><p>did data augmentation on training generator, validation generator</p><p></p>|- Given the image of a cat or a dog, the model predicts  whether if it being a cat or dog |
|DL|[Churn prediction](https://github.com/RamSankarTheDeveloper/projects/blob/main/5.deep learning/ANN/2.ANN tutorial churn prediction/ANN_churn_prediction.ipynb)|Churn\_modelling.csv dataset from kaggle|<p>ANN</p><p>(84.25)</p>|tensorflow, keras, sklearn, numpy, pandas, matplotlib|<p>- Did some analysis and preprocessing on dataset</p><p>- Created an ANN blackbox model with 3 layers and trained it with the preprocessed dataset</p><p>- Analysed the accuracy of  the model</p>|- Given numerical datapoint about a customer, the model predicts the probability of the customer terminating his/her account from the bank|
|DL|[Handwritten digit recognition](https://github.com/RamSankarTheDeveloper/projects/tree/main/5.deep learning/ANN/3.handwritten digits)|Mnist dataset from keras|<p>ANN</p><p>(65.68)</p>|keras, numpy, matplotlib|<p>- Imported dataset</p><p>- Created a  3 layer black box model, trained using dataset, error happened due to incompatible shapes</p><p>- Reshaped using one hot encoding</p><p>- Trained successfully</p><p>- Evaluated the model</p>|- Given pixel values in 28\*28 resoultion (B&W) of an image of handwritten number(single digit), the model will predict the number|
|DL|[Diabetics prediction](https://github.com/RamSankarTheDeveloper/projects/tree/main/5.deep learning/ANN/1.diabetis)|Diabetis dataset from Kaggle(USA, before 1990)|<p>ANN</p><p>(74.61)</p>|keras, pandas, numpy|<p>- Created an ANN blackbox model with 3 layers and trained it with the dataset</p><p>- Analysed the accuracy of  the model</p>|- Given the numerical data, determines the probability that the patient has diabetis and not|
|NLP|[Hate speech detection](https://github.com/RamSankarTheDeveloper/projects/tree/main/4.Natural Language processing/twitter hate speech)|Twitter data|MultinomialNB, Random forest classifier, XGBclassifier,SVM, Logistic regression|sklearn and other data analysis libraries|<p>- Did analysis and preprocessing</p><p>- Applied Regex to clean data</p><p>- Applied oversampling (random)</p><p>- Applied Tfidftransformer  to xtrain after applying Count-vectorizer</p><p>- Did multiple algorithms and analysed each model’s F1 score</p><p>- Analysed ROC curve, Precision-recall curve for Random forest and XGBoost</p>|- Given comment/sentence, the model will predict if it is hate speech or not (1 if hate speech, 0 if not)|
|NLP|[Sentiment analysis using text classification](https://github.com/RamSankarTheDeveloper/projects/tree/main/4.Natural Language processing/sentiment analysis project)|Sentiment analysis of movie reviews dataset from Kaggle|<p>Multinomial naive bayes</p><p>(58.65)</p>|sklearn, pandas, matplotlib|<p>- Did some analysis on dataset</p><p>- Applied Tfidfvectorizer</p><p>- Fitted the model</p><p>- Tested the model</p>|- Given a review that was preprocessed with Tfidfvectorizer, the model will predict: 0 if negative, 1 if somewhat negative, 2 if neutral, 3 if somewhat positive, 4 if positive|
|NLP|[Email spam classification](https://github.com/RamSankarTheDeveloper/projects/tree/main/2.ML/Supervised learning/others)|Smsspamcollection dataset|<p>Multinomial naive bayes</p><p>(WA of f1=0.98)</p>|<p>sklearn, joblib, pandas, imblearn</p><p>(add details?)</p>|<p>- Preprocessed dataset with Encoding, Countvectorizer, Random sampling</p><p>- Fitted the model</p><p>- Analysed the model with Classification matrix</p><p>- Converted the model to Joblib format and saved to file</p>|- Preprocess a sample into countvectorizer feed to model and the model will predict if it is a spam comment or not|
|Computervision|[Face detector & recognizer](https://github.com/RamSankarTheDeveloper/projects/tree/main/3.computer vision/face_recognition)|Self made data applied on pretrained models|HAAR cascade pretrained model,LBPH face recognizer|cv2, os, csv, numpy, PIL, pandas|<p>- Created folders ‘id’ and ‘name’</p><p>- Took face samples directly from video feed detected by HAAR cascade detector and stored as file</p><p>- Created a subdirectory for trainer data, cropped out the faces using haar, trained using LBPH Recognizer</p><p>- Saved the trained model(yml file) into subdirectory,’trainer’</p><p>- Tested the model with opening Camera,  preprocessed the feed, and analysed using c-Confidence of the Recognizer.</p>|- selfie|
|ML|[Customer segmentation](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Unsupervised learning/clustering/DBSCAN VISUALIZED detailed.ipynb)|Mall\_customers dataset from Kaggle|K-means clustering, DBSCAN clustering, Agglomerative clustering, Hierarchical clustering|sklearn, pandas, matplotlib, numpy, math|<p>- Feature engineered necessary columns</p><p>- Fitted K-means algorithm 10 times with 10 values</p><p>- Plotted elbow method graph (wcss list vs number of clusters) and chose optimum number of clusters</p><p>- Visualised clusters</p><p>- Used nearest neighbors to determine  optimum parameters with K-distance graph, fitted on DBSCAN  and visualised the clusters</p><p>- Fitted Agglomerative and Hierarchical clustering additionally</p>|- Given income, it will predict the cluster the person is part of|
|ML|[Market basket  analysis of store data](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Unsupervised learning/association/apriori algorithm of market basket analysis.ipynb)|7500 transactions of a week at a French retail store|Apriori|apyori, numpy, pandas, matplotlib|<p>- Analysed and preprocessed data into list for Apriori algorithm</p><p>- Fitted Apriori model with required parameters</p><p>- Derived relations</p><p>- Derived rules</p>|- The model created some rules. You can analyse the relatons of items with other items.|
|ML|[Iris multiclass classification](https://github.com/RamSankarTheDeveloper/projects/tree/main/2.ML/Supervised learning/classification/multiclass classification)|Iris dataset(the characteristics of each iris species flowers)|Decision trees(0.93), KNN(0.93), Random forest(0.93), Gradient boosting regressor(0.93)|matplotlib, seaborn, pandas, numpy|<p>- Performed data analysis and visualisation on dataset</p><p>- Trained decision tree model, visualised tree, evaluated, hyperparameter tuned with K fold and GridSearchCV</p><p>- Trained KNN, Random forest, Gradient boosting regressor models</p><p>- Hyperparameter tuned the above models</p><p>- Evaluated the above models with multilabel confusion matrix and classification report.</p>|- Given indipendent variables, models will predict the class the sample datapoint belongs in.|
|ML|[Breast cancer prediction](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Supervised learning/classification/binary classification/rest of binary classifications/rest of binary classifications.ipynb)|Breast cancer dataset|Logistic regression, SVM, KNN, Decision tree, Naive bayes|sklearn, matplotlib, pandas|<p>- Dataset splitted into train and test</p><p>- Scaled independent variables</p><p>- Trained models of Logistic regression, SVM, KNN, Decision tree, Naive bayes</p><p>- Evaluated all the models and visually compared the classification report. Logistic regression have the most overall performance at the moment</p>|- Given indipendent variables, models will predict the  class the sample datapoint belongs in.|
|ML|[Predicting survival Titanic passengers](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Supervised learning/classification/binary classification/logistic regression/logistic regression.ipynb)|Titanic dataset|Logistic regression (79.88)|sklearn, pandas|<p>- Analysed and preprocessed the datset with Pandas</p><p>- Dataset splitted into train and test</p><p>- Fitted Logistic regression</p><p>- Analysed the model by evaluating the accuracy score</p>|- given the indipendent variables of passengers in Titanic dataset, it will predict whether they survive or not|
|ML|[Predicting car purchase customers](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Supervised learning/classification/binary classification/logistic regression/logistic regression 2.ipynb)|SUV dataset(whether the customer buys SUV or not)|<p>Logistic regression</p><p>(WA= 0.83)</p>|sklearn, pandas, matplotlib, seaborn|<p>- Analysed and preprocessed dataset</p><p>- Logistic model is created and fitted</p><p>- Analysed with Classification matrix</p><p>- Plotted Roc-auc curve</p><p>- Calculatted Log loss(0.39)</p>|- Given the indipendent variables of customers, the model will predict whether they will buy the car or not|
|ML|[Brain weight prediction](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Supervised learning/regression/linear regression.ipynb)|Headbrain.csv|<p>Linear regression</p><p>(R2=0.63)</p>|sklearn, numpy, pandas|<p>- Splitted dataset into train-test</p><p>- Fitted into Linear regression model</p><p>- Evaluated score</p>|- Given input variables, model will try to predict a value|
|ML|[Polynomial regression from scratch without  vectorization](https://github.com/RamSankarTheDeveloper/projects/blob/main/2.ML/Supervised learning/regression/polynomial self.ipynb)|Headbrain.csv|<p>Polynomial regression without dot product</p><p>(R2=0.58)</p>|sklearn, numpy, pandas, matplotlib|<p>- Converted data into input and output</p><p>- Standardized indipendent values</p><p>- Executed program and Regrerssion line visualized</p><p>- Evaluated model with Error-iteration plot</p><p>- Calculated R2 score</p>|- Given input variables, model will try to predict a value|
|Data Analysis|<p>FB posts analysis</p><p></p>||||||
|Data Analysis|Titanic dataset analysis||||||
|Data Analysis|‘Times of India’Newspaper analysis||||||

